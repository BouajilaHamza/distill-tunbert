{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f1dadc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3bc941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset, get_dataset_config_names\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Check if the datasets library is installed, and install it if not.\n",
    "try:\n",
    "    import datasets\n",
    "except ImportError:\n",
    "    print(\"The 'datasets' library is not installed. Installing now...\")\n",
    "    os.system(\"pip install datasets\")\n",
    "    from datasets import load_dataset, concatenate_datasets, Dataset, get_dataset_config_names\n",
    "\n",
    "# --- Dataset Configuration ---\n",
    "# List of datasets to load from Hugging Face.\n",
    "HF_DATASETS_TO_LOAD = [\n",
    "    \"AzizBelaweid/Tunisian_Language_Dataset\",\n",
    "    \"arbml/Tunisian_Dialect_Corpus\",\n",
    "    \"hamzabouajila/Sample_Tunisiya_Dataset\",\n",
    "    \"abdouuu/tunisian_chatbot_data\",\n",
    "    \"linagora/Tunisian_Derja_Dataset\",\n",
    "]\n",
    "\n",
    "# The name of the column that contains the text.\n",
    "TEXT_COLUMN_NAME = \"text\"\n",
    "# A list of possible text column names to check for.\n",
    "POSSIBLE_TEXT_COLUMN_NAMES = [\"text\", \"content\", \"snippet\", \"description\", \"tweets\", \"sentence\", \"Expanded Context\", \"input\", \"instruction\"]\n",
    "\n",
    "# --- Main Script Functions ---\n",
    "\n",
    "def find_text_column(dataset_columns):\n",
    "    \"\"\"Finds the most suitable text column from a list of column names.\"\"\"\n",
    "    for col in POSSIBLE_TEXT_COLUMN_NAMES:\n",
    "        if col in dataset_columns:\n",
    "            return col\n",
    "    # Fallback to a case-insensitive check\n",
    "    for col in dataset_columns:\n",
    "        if col.lower() in [s.lower() for s in POSSIBLE_TEXT_COLUMN_NAMES]:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def load_and_preprocess_datasets():\n",
    "    \"\"\"\n",
    "    Loads specified datasets, renames their text columns for consistency,\n",
    "    and returns a list of loaded datasets.\n",
    "    \"\"\"\n",
    "    all_datasets = []\n",
    "    \n",
    "    print(\"Starting to load datasets from Hugging Face...\")\n",
    "    for dataset_name in tqdm(HF_DATASETS_TO_LOAD, desc=\"Loading datasets\"):\n",
    "        try:\n",
    "            # Handle the specific case for the uploaded XLSX file\n",
    "            if dataset_name == \"hamzabouajila/Sample_Tunisiya_Dataset\":\n",
    "                print(f\"Loading specific CSV file for {dataset_name} using pandas.\")\n",
    "                snapshot_download(\"hamzabouajila/Sample_Tunisiya_Dataset\",local_dir=\"hamzabouajila/Sample_Tunisiya_Dataset\")\n",
    "                df = pd.read_csv(\"search_results.xlsx\")\n",
    "                ds = Dataset.from_pandas(df)\n",
    "      \n",
    "            \n",
    "            # Handle the specific case for the linagora dataset with multiple configs\n",
    "            elif dataset_name == \"linagora/Tunisian_Derja_Dataset\":\n",
    "                print(get_dataset_split_names(\"linagora/Tunisian_Derja_Dataset\", config_name=\"Tunisian_Dialectic_English_Derja\"))\n",
    "\n",
    "                print(f\"Loading all configurations for {dataset_name}.\")\n",
    "                configs = get_dataset_config_names(dataset_name)\n",
    "                for config in configs:\n",
    "                    print(f\"  - Loading config: {config}\")\n",
    "                    ds = load_dataset(dataset_name, config, split=\"train\")\n",
    "                    \n",
    "                    if TEXT_COLUMN_NAME not in ds.column_names:\n",
    "                        found_text_column = find_text_column(ds.column_names)\n",
    "                        if found_text_column:\n",
    "                            ds = ds.rename_column(found_text_column, TEXT_COLUMN_NAME)\n",
    "                        else:\n",
    "                            print(f\"Warning: Could not find a suitable text column in config '{config}'. Skipping.\")\n",
    "                            continue\n",
    "                    \n",
    "                    all_datasets.append(ds.select_columns([TEXT_COLUMN_NAME]))\n",
    "                continue # Move to the next dataset in the main loop\n",
    "\n",
    "            else:\n",
    "                # Load other datasets directly.\n",
    "                ds = load_dataset(dataset_name, split=\"train\")\n",
    "            \n",
    "            # Check for the text column and rename it to a standard name.\n",
    "            if TEXT_COLUMN_NAME not in ds.column_names:\n",
    "                found_text_column = find_text_column(ds.column_names)\n",
    "                if found_text_column:\n",
    "                    print(f\"Renaming '{found_text_column}' to '{TEXT_COLUMN_NAME}' in {dataset_name}.\")\n",
    "                    ds = ds.rename_column(found_text_column, TEXT_COLUMN_NAME)\n",
    "                else:\n",
    "                    print(f\"Warning: Could not find a suitable text column in {dataset_name}. Skipping.\")\n",
    "                    continue\n",
    "            \n",
    "            all_datasets.append(ds.select_columns([TEXT_COLUMN_NAME]))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {dataset_name}: {e}. Skipping this dataset.\")\n",
    "    \n",
    "    return all_datasets\n",
    "\n",
    "def combine_and_deduplicate(datasets_list):\n",
    "    \"\"\"\n",
    "    Combines all datasets into a single one, removes duplicates, and shuffles.\n",
    "    \"\"\"\n",
    "    if not datasets_list:\n",
    "        print(\"No datasets to combine. Exiting.\")\n",
    "        return None\n",
    "        \n",
    "    print(\"\\nConcatenating datasets...\")\n",
    "    \n",
    "    # Filter out streaming datasets for concatenation\n",
    "    non_streaming_datasets = [ds for ds in datasets_list if not isinstance(ds, datasets.iterable_dataset.IterableDataset)]\n",
    "    \n",
    "    if not non_streaming_datasets:\n",
    "        print(\"No non-streaming datasets to combine. Exiting.\")\n",
    "        return None\n",
    "\n",
    "    combined_dataset = concatenate_datasets(non_streaming_datasets)\n",
    "    \n",
    "    print(f\"Initial combined dataset size: {len(combined_dataset)} rows.\")\n",
    "    \n",
    "    print(\"Removing duplicate rows...\")\n",
    "    df = combined_dataset.to_pandas()\n",
    "    original_size = len(df)\n",
    "    df.drop_duplicates(subset=[TEXT_COLUMN_NAME], inplace=True)\n",
    "    deduplicated_size = len(df)\n",
    "    print(f\"Removed {original_size - deduplicated_size} duplicates. Final size: {deduplicated_size} rows.\")\n",
    "    \n",
    "    # Convert back to a Hugging Face dataset.\n",
    "    final_dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "    print(\"Shuffling the final dataset...\")\n",
    "    final_dataset = final_dataset.shuffle(seed=42)\n",
    "    \n",
    "    return final_dataset\n",
    "\n",
    "def save_dataset(dataset, output_path=\"tunbert_distillation_corpus.parquet\"):\n",
    "    \"\"\"\n",
    "    Saves the final prepared dataset to a Parquet file.\n",
    "    \"\"\"\n",
    "    if dataset:\n",
    "        print(f\"\\nSaving the final dataset to '{output_path}'...\")\n",
    "        dataset.to_parquet(output_path)\n",
    "        print(\"Dataset saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f0041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset, get_dataset_config_names, get_dataset_split_names\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Check if the datasets library is installed, and install it if not.\n",
    "try:\n",
    "    import datasets\n",
    "except ImportError:\n",
    "    print(\"The 'datasets' library is not installed. Installing now...\")\n",
    "    os.system(\"pip install datasets\")\n",
    "    from datasets import load_dataset, concatenate_datasets, Dataset, get_dataset_config_names, get_dataset_split_names\n",
    "\n",
    "# --- Dataset Configuration ---\n",
    "# List of datasets to load from Hugging Face.\n",
    "HF_DATASETS_TO_LOAD = [\n",
    "    \"AzizBelaweid/Tunisian_Language_Dataset\",\n",
    "    \"arbml/Tunisian_Dialect_Corpus\",\n",
    "    \"hamzabouajila/Sample_Tunisiya_Dataset\",\n",
    "    \"abdouuu/tunisian_chatbot_data\",\n",
    "    \"linagora/Tunisian_Derja_Dataset\",\n",
    "]\n",
    "\n",
    "# The name of the column that contains the text.\n",
    "TEXT_COLUMN_NAME = \"text\"\n",
    "# A list of possible text column names to check for.\n",
    "POSSIBLE_TEXT_COLUMN_NAMES = [\"text\", \"content\", \"snippet\", \"description\", \"tweets\", \"sentence\", \"Expanded Context\", \"input\", \"instruction\"]\n",
    "\n",
    "# --- Main Script Functions ---\n",
    "\n",
    "def find_text_column(dataset_columns):\n",
    "    \"\"\"Finds the most suitable text column from a list of column names.\"\"\"\n",
    "    for col in POSSIBLE_TEXT_COLUMN_NAMES:\n",
    "        if col in dataset_columns:\n",
    "            return col\n",
    "    # Fallback to a case-insensitive check\n",
    "    for col in dataset_columns:\n",
    "        if col.lower() in [s.lower() for s in POSSIBLE_TEXT_COLUMN_NAMES]:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def ensure_text_column_type(dataset, dataset_name):\n",
    "    \"\"\"\n",
    "    Ensures the text column is of string type to avoid type mismatch during concatenation.\n",
    "    \"\"\"\n",
    "    if TEXT_COLUMN_NAME in dataset.column_names:\n",
    "        # Convert to pandas to check and convert data type\n",
    "        df = dataset.to_pandas()\n",
    "        if df[TEXT_COLUMN_NAME].dtype != \"object\":  # Check if not string-like\n",
    "            print(f\"Converting '{TEXT_COLUMN_NAME}' column to string in {dataset_name} (original type: {df[TEXT_COLUMN_NAME].dtype}).\")\n",
    "            df[TEXT_COLUMN_NAME] = df[TEXT_COLUMN_NAME].astype(str).replace(\"nan\", None)\n",
    "            dataset = Dataset.from_pandas(df)\n",
    "    return dataset\n",
    "\n",
    "def load_and_preprocess_datasets():\n",
    "    \"\"\"\n",
    "    Loads specified datasets, renames their text columns for consistency,\n",
    "    ensures text column is string type, and returns a list of loaded datasets.\n",
    "    \"\"\"\n",
    "    all_datasets = []\n",
    "    hf_token = os.getenv(\"HF_TOKEN\")  # Get Hugging Face token for authentication\n",
    "    \n",
    "    print(\"Starting to load datasets from Hugging Face...\")\n",
    "    for dataset_name in tqdm(HF_DATASETS_TO_LOAD, desc=\"Loading datasets\"):\n",
    "        try:\n",
    "            if dataset_name == \"hamzabouajila/Sample_Tunisiya_Dataset\":\n",
    "                print(f\"Loading specific CSV file for {dataset_name} using pandas.\")\n",
    "                snapshot_download(repo_id=dataset_name,\n",
    "                                 repo_type=\"dataset\",\n",
    "                                 local_dir=\"hamzabouajila/Sample_Tunisiya_Dataset\",\n",
    "                                 token=hf_token)\n",
    "                file_path = \"hamzabouajila/Sample_Tunisiya_Dataset/search_results.xlsx\"\n",
    "                if not os.path.exists(file_path):\n",
    "                    print(f\"Error: File {file_path} not found. Skipping {dataset_name}.\")\n",
    "                    continue\n",
    "                df = pd.read_csv(file_path,encoding='utf-8')\n",
    "                # Ensure the text column exists and is string type\n",
    "                text_col = find_text_column(df.columns)\n",
    "                if text_col:\n",
    "                    print(f\"Found text column '{text_col}' in {dataset_name}.\")\n",
    "                    df = df.rename(columns={text_col: TEXT_COLUMN_NAME})\n",
    "                    df[TEXT_COLUMN_NAME] = df[TEXT_COLUMN_NAME].astype(str).replace(\"nan\", None)\n",
    "                    ds = Dataset.from_pandas(df)\n",
    "                else:\n",
    "                    print(f\"Warning: Could not find a suitable text column in {dataset_name}. Skipping.\")\n",
    "                    continue\n",
    "            \n",
    "            # Handle the specific case for the linagora dataset with multiple configs\n",
    "            elif dataset_name == \"linagora/Tunisian_Derja_Dataset\":\n",
    "                print(f\"Loading all configurations for {dataset_name}.\")\n",
    "                configs = get_dataset_config_names(dataset_name, token=hf_token)\n",
    "                for config in configs:\n",
    "                    try:\n",
    "                        print(f\"  - Loading config: {config}\")\n",
    "                        # Check available splits\n",
    "                        available_splits = get_dataset_split_names(dataset_name, config_name=config, token=hf_token)\n",
    "                        print(f\"    Available splits: {available_splits}\")\n",
    "                        split_to_use = \"train\" if \"train\" in available_splits else available_splits[0] if available_splits else None\n",
    "                        \n",
    "                        if not split_to_use:\n",
    "                            print(f\"    No valid splits found for config '{config}'. Skipping.\")\n",
    "                            continue\n",
    "                        \n",
    "                        ds = load_dataset(dataset_name, config, split=split_to_use, token=hf_token)\n",
    "                        \n",
    "                        if TEXT_COLUMN_NAME not in ds.column_names:\n",
    "                            found_text_column = find_text_column(ds.column_names)\n",
    "                            if found_text_column:\n",
    "                                print(f\"    Renaming '{found_text_column}' to '{TEXT_COLUMN_NAME}' in config {config}.\")\n",
    "                                ds = ds.rename_column(found_text_column, TEXT_COLUMN_NAME)\n",
    "                            else:\n",
    "                                print(f\"    Warning: Could not find a suitable text column in config '{config}'. Skipping.\")\n",
    "                                continue\n",
    "                        \n",
    "                        # Ensure text column is string type\n",
    "                        ds = ensure_text_column_type(ds, f\"{dataset_name}/{config}\")\n",
    "                        all_datasets.append(ds.select_columns([TEXT_COLUMN_NAME]))\n",
    "                        print(f\"    Successfully loaded config '{config}' with {len(ds)} rows.\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"    An error of type {type(e).__name__} occurred while loading config '{config}' for {dataset_name}. Details: {e}. Skipping this config.\")\n",
    "                continue  # Move to the next dataset in the main loop\n",
    "\n",
    "            else:\n",
    "                # Load other datasets directly.\n",
    "                available_splits = get_dataset_split_names(dataset_name, token=hf_token)\n",
    "                print(f\"Available splits for {dataset_name}: {available_splits}\")\n",
    "                split_to_use = \"train\" if \"train\" in available_splits else available_splits[0] if available_splits else None\n",
    "                \n",
    "                if not split_to_use:\n",
    "                    print(f\"No valid splits found for {dataset_name}. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                ds = load_dataset(dataset_name, split=split_to_use, token=hf_token)\n",
    "            \n",
    "            # Check for the text column and rename it to a standard name.\n",
    "            if TEXT_COLUMN_NAME not in ds.column_names:\n",
    "                found_text_column = find_text_column(ds.column_names)\n",
    "                if found_text_column:\n",
    "                    print(f\"Renaming '{found_text_column}' to '{TEXT_COLUMN_NAME}' in {dataset_name}.\")\n",
    "                    ds = ds.rename_column(found_text_column, TEXT_COLUMN_NAME)\n",
    "                else:\n",
    "                    print(f\"Warning: Could not find a suitable text column in {dataset_name}. Skipping.\")\n",
    "                    continue\n",
    "            \n",
    "            # Ensure text column is string type\n",
    "            ds = ensure_text_column_type(ds, dataset_name)\n",
    "            all_datasets.append(ds.select_columns([TEXT_COLUMN_NAME]))\n",
    "            print(f\"Successfully loaded {dataset_name} with {len(ds)} rows.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error of type {type(e).__name__} occurred while loading {dataset_name}. Details: {e}. Skipping this dataset.\")\n",
    "    \n",
    "    return all_datasets\n",
    "\n",
    "def combine_and_deduplicate(datasets_list):\n",
    "    \"\"\"\n",
    "    Combines all datasets into a single one, removes duplicates, and shuffles.\n",
    "    \"\"\"\n",
    "    if not datasets_list:\n",
    "        print(\"No datasets to combine. Exiting.\")\n",
    "        return None\n",
    "        \n",
    "    print(\"\\nConcatenating datasets...\")\n",
    "    \n",
    "    # Filter out streaming datasets for concatenation\n",
    "    non_streaming_datasets = [ds for ds in datasets_list if not isinstance(ds, datasets.iterable_dataset.IterableDataset)]\n",
    "    \n",
    "    if not non_streaming_datasets:\n",
    "        print(\"No non-streaming datasets to combine. Exiting.\")\n",
    "        return None\n",
    "\n",
    "    # Log feature types for debugging\n",
    "    for i, ds in enumerate(non_streaming_datasets):\n",
    "        print(f\"Dataset {i+1} features: {ds.features}\")\n",
    "\n",
    "    combined_dataset = concatenate_datasets(non_streaming_datasets)\n",
    "    \n",
    "    print(f\"Initial combined dataset size: {len(combined_dataset)} rows.\")\n",
    "    \n",
    "    print(\"Removing duplicate rows...\")\n",
    "    df = combined_dataset.to_pandas()\n",
    "    original_size = len(df)\n",
    "    df.drop_duplicates(subset=[TEXT_COLUMN_NAME], inplace=True)\n",
    "    deduplicated_size = len(df)\n",
    "    print(f\"Removed {original_size - deduplicated_size} duplicates. Final size: {deduplicated_size} rows.\")\n",
    "    \n",
    "    # Convert back to a Hugging Face dataset.\n",
    "    final_dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "    print(\"Shuffling the final dataset...\")\n",
    "    final_dataset = final_dataset.shuffle(seed=42)\n",
    "    \n",
    "    # Print a sample for debugging\n",
    "    print(f\"Sample from combined dataset: {final_dataset[:5][TEXT_COLUMN_NAME]}\")\n",
    "    \n",
    "    return final_dataset\n",
    "\n",
    "def save_dataset(dataset, output_path=\"tunbert_distillation_corpus.parquet\"):\n",
    "    \"\"\"\n",
    "    Saves the final prepared dataset to a Parquet file.\n",
    "    \"\"\"\n",
    "    if dataset:\n",
    "        print(f\"\\nSaving the final dataset to '{output_path}'...\")\n",
    "        dataset.to_parquet(output_path)\n",
    "        print(\"Dataset saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7b0aa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to load datasets from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "395abe31199e4b319f175bc6f90b99f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading datasets:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available splits for AzizBelaweid/Tunisian_Language_Dataset: ['train']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded AzizBelaweid/Tunisian_Language_Dataset with 269773 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available splits for arbml/Tunisian_Dialect_Corpus: ['train']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not find a suitable text column in arbml/Tunisian_Dialect_Corpus. Skipping.\n",
      "Loading specific CSV file for hamzabouajila/Sample_Tunisiya_Dataset using pandas.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7e9e2030ec4bfc90d51d1dea9334d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error of type UnicodeDecodeError occurred while loading hamzabouajila/Sample_Tunisiya_Dataset. Details: 'utf-8' codec can't decode byte 0xaf in position 10: invalid start byte. Skipping this dataset.\n",
      "Available splits for abdouuu/tunisian_chatbot_data: ['train']\n",
      "Renaming 'input' to 'text' in abdouuu/tunisian_chatbot_data.\n",
      "Converting 'text' column to string in abdouuu/tunisian_chatbot_data (original type: float64).\n",
      "Successfully loaded abdouuu/tunisian_chatbot_data with 1426 rows.\n",
      "Loading all configurations for linagora/Tunisian_Derja_Dataset.\n",
      "  - Loading config: Derja_tunsi\n",
      "    Available splits: ['train']\n",
      "    Successfully loaded config 'Derja_tunsi' with 13037 rows.\n",
      "  - Loading config: HkayetErwi\n",
      "    Available splits: ['train']\n",
      "    Successfully loaded config 'HkayetErwi' with 946 rows.\n",
      "  - Loading config: Sentiment_Derja\n",
      "    Available splits: ['train']\n",
      "    Successfully loaded config 'Sentiment_Derja' with 22890 rows.\n",
      "  - Loading config: TA_Segmentation\n",
      "    Available splits: ['train']\n",
      "    Successfully loaded config 'TA_Segmentation' with 30530 rows.\n",
      "  - Loading config: TSAC\n",
      "    Available splits: ['train']\n",
      "    Successfully loaded config 'TSAC' with 8416 rows.\n",
      "  - Loading config: TuDiCOI\n",
      "    Available splits: ['train']\n",
      "    Successfully loaded config 'TuDiCOI' with 3426 rows.\n",
      "  - Loading config: TunBERT\n",
      "    Available splits: ['train']\n",
      "    Successfully loaded config 'TunBERT' with 67186 rows.\n",
      "  - Loading config: TunSwitchCodeSwitching\n",
      "    Available splits: ['train']\n",
      "    Successfully loaded config 'TunSwitchCodeSwitching' with 394160 rows.\n",
      "  - Loading config: TunSwitchTunisiaOnly\n",
      "    Available splits: ['train']\n",
      "    Successfully loaded config 'TunSwitchTunisiaOnly' with 380546 rows.\n",
      "  - Loading config: TunisianSentimentAnalysis\n",
      "    Available splits: ['train']\n",
      "    Successfully loaded config 'TunisianSentimentAnalysis' with 23786 rows.\n",
      "  - Loading config: Tunisian_Dialectic_English_Derja\n",
      "    Available splits: ['Tunisian_Dialectic_English_Derja']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1d1c7c643c404091fcb0d7ed7e9612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    An error of type ExpectedMoreSplitsError occurred while loading config 'Tunisian_Dialectic_English_Derja' for linagora/Tunisian_Derja_Dataset. Details: {'Tunisian_Dialectic_English_Derja'}. Skipping this config.\n",
      "  - Loading config: Tweet_TN\n",
      "    Available splits: ['train']\n",
      "    Successfully loaded config 'Tweet_TN' with 39637 rows.\n",
      "\n",
      "Concatenating datasets...\n",
      "Dataset 1 features: {'text': Value('string')}\n",
      "Dataset 2 features: {'text': Value('null')}\n",
      "Dataset 3 features: {'text': Value('string')}\n",
      "Dataset 4 features: {'text': Value('string')}\n",
      "Dataset 5 features: {'text': Value('string')}\n",
      "Dataset 6 features: {'text': Value('string')}\n",
      "Dataset 7 features: {'text': Value('string')}\n",
      "Dataset 8 features: {'text': Value('string')}\n",
      "Dataset 9 features: {'text': Value('string')}\n",
      "Dataset 10 features: {'text': Value('string')}\n",
      "Dataset 11 features: {'text': Value('string')}\n",
      "Dataset 12 features: {'text': Value('string')}\n",
      "Dataset 13 features: {'text': Value('string')}\n",
      "Initial combined dataset size: 1255759 rows.\n",
      "Removing duplicate rows...\n",
      "Removed 470050 duplicates. Final size: 785709 rows.\n",
      "Shuffling the final dataset...\n",
      "Sample from combined dataset: ['الأمازيغ يحتفلون اليوم بقدوم سنة 2969 - Tounes 24\\nازمة الكورونا .. وزير الصحة : \" تونس لم تدخل...\\nMar 30, 2020 521\\nإستئناف الدروس بالمؤسسات التعليمية أبرز محاور...\\nMar 30, 2020 301\\nتسجيل 5 إصابات جديدة بفيروس كورونا في ليبيا\\nMar 29, 2020 72\\nDec 5, 2019 499\\nNov 29, 2019 576\\nMar 28, 2020 146\\nMar 27, 2020 545\\nMar 25, 2020 375\\nصورة اليوم : شرطي يرتدي قناع كورونا لتخويف أصحاب...\\nترامب : إذا تحكمنا في وفايات كورونا في حدود 100...\\nهيثم المكي : النائب مبروك كورشيد صاحب مجلة إلكترونية...\\nMar 30, 2020 269\\nهيثم المكي: باعثي القنوات التلفزية ماعندهم حتى...\\nسمير بن عمر لمبروك كورشيد :إذا عدت أيها الفاسد...\\nMar 30, 2020 313\\nMar 30, 2020 382\\nMar 30, 2020 422\\nMar 30, 2020 1022\\nكيف يقتل الجسم نفسه في محاولة الشفاء من الانفلونزا...\\nعلى عكس المتداول... فيروس كورونا يموت بحرارة...\\nMar 20, 2020 363\\nFeb 28, 2020 794\\nFeb 28, 2020 266\\nFeb 21, 2020 216\\nMar 27, 2020 379\\nMar 27, 2020 1311\\nMar 26, 2020 715\\nJan 12, 2019 617\\nيحتفل الأمازيغ، الذين يتركزون بالأساس في شمال أفريقيا، برأس السنة الجديدة 2969 بأطباق خاصة وعادات تنتصر للتراث الأمازيغي والأرض والزراعة.\\nينقسم المؤرخون حول أصل الاحتفال برأس السنة الأمازيغية إلى فريقين، فريق يرى أن اختيار هذا التاريخ من جانفي يرمز للاحتفال بالأرض والزراعة، ما جعلها معروفة باسم \"السنة الفلاحية\". أما الفريق الثاني، فيربطها بالاحتفال بذكرى اليوم الذي انتصر فيه الملك الأمازيغي \"شاشناق\" على الفرعون المصري \"رمسيس الثاني\" في مصر.\\nتعتبر الجزائر أول دولة في شمال أفريقيا تقر رأس السنة الأمازيغية عطلة رسمية، إذ أصدر الرئيس الجزائري، عبد العزيز بوتفليقة، نهاية ديسمبرالماضي قراراً باعتبار 12 جانفي عطلةً وطنية رسمية مدفوعة الأجر، في سياق \"جهود الدولة الرامية إلى ترقية التراث واللغة الأمازيغية\".\\nسنة امازيغية\\nمونديال كرة اليد : برنامج مباريات اليوم الثالث\\nتعرّف إلى أصغر ملكة في العالم (صور )\\nعنوان \"سامي الفهري يغازل لطفي العبدلي\" يثير سخرية سمير...\\nAug 17, 2019 8488\\nرغم انه دائم الغياب عن الجلسات .. رضا شرف الدين مرشح قلب...\\nNov 7, 2019 138\\nدرة زروق امام القضاء بسبب تهمة التخابر مع جهات أجنبية...\\nSep 23, 2019 2632\\nTotal Vote: 5240\\nNov 17, 2018 154673\\nMar 22, 2020 101942\\nFeb 14, 2019 68029\\nJan 18, 2019 67109\\nMar 25, 2020 58328\\nوزير الصحة : فترة استمرار الحجر الصحي .. وقد تستمر 4 أشهر...\\nMar 29, 2020 913\\nالدكتور لهيذب : تونس على وشك الخروج من منطقة الخطر...واصلوا\\nMar 29, 2020 8830\\nالفلكية اللبنانية التي قالت بان تونس ستصبح مثل دبي وتوقعت...\\nMar 29, 2020 7381\\nMar 28, 2020 1614\\nالعرباوي : النّهضة لا تعارض حصول التيّار على وزارتي العدل...\\nDec 17, 2019 244\\nالعرباوي : النّهضة لا تعارض حصول التيّار على وزارتي العدل والإصلاح الإداري لكن...\\nعبد المنعم شويات يتحصل الجائزة الكبرى لمسابقة « Sotigui...\\nDec 5, 2018 390\\nالأمطار في صفاقس : امرأة تلقى حتفها بعد لمسها لعمود كهربائي...\\nوفاة مواطنة تبلغ من العمر 35 سنة في ولاية صفاقس جراء لمسها لعمود كهربائي إثر تهاطل...\\nميناء رادس : إحباط عمليّة توريدٍ دون إعلام لسجائر إلكترونيّة...\\nApr 17, 2019 203\\nيوسف الشاهد يعلّق على وجود نبيل القروي في السجن\\nSep 12, 2019 1805\\nالطيب بالصادق\\nورم دماغ\\nسيف الدين مخلوق\\nعبد الرحمان الخشتالي\\nالسبسي يلتقي فائز السراج حول العمليات الإرهابية لخليفة...\\nالطبوبي: \"اخترت الرئيس القادم الذي ينبذ التفرقة والكلام...', 'حتي ملحدين الحاسيلو م الاخر اختكم بنت عايلة اليوم ربي و باني لقيت الحل إلي يرضى جميع الاطراف علمانيين و اصوليين', 'المظيلة .. تعرض عون بشركة فسفاط قفصة الى حادث شغل بالمغسلة عدد2 - الصباح نيوز | Assabah News\\nالكاف.. 45 تونسي عالقون بمدينة تبسة الجزائرية\\n2020/4/7 10:20\\nمازال حوالي 45 فرد من مهندسين وفنيين وعملة تونسيين عالقين بمدينة تبسة الجزائرية منذ21مارس المنقضي. رغم وعود رئيس الجمهورية ورئيس الحكومة بالتسريع بإدخال جميع العالقين خارج ارض الوطن . وهم يتواجدون إلى اليوم فى ظروف قاسية بأحدى النزل الشعبية بمدينة تبسة دون اية إحاطة من القنصلية...\\nفي انتظار رفع التحاليل.. إخضاع العاملين في معتمدية سوسة جوهرة للحجر الصحي\\n2020/4/7 10:19\\nعلمت \"الصباح نيوز \" أنه وعلى خلفية ثبوت حالة إصابة مأكدة بفيروس كورونا لدى مواطنة كانت قد كسرت الحجر الصحي الذاتي وقامت يوم أمس بالتوجه إلى معتمدية سوسة جوهرة والوقوف في طابور الراغبين في الانتفاع بالمساعدات الاجتماعية الظرفية والاستثنائية فقد اتخذت السلط الجهوية قرارا بإخضاع...\\nسوسة.. إيقاف صاحب شاحنة خالف قانون الحجر الصحي\\n2020/4/7 10:09\\nنجحت دورية أمنية تابعة لمركز الأمن الوطني بسيدي بوعلي من حجز سيارة نوع\"ديماكس\" وايقاف صاحبها بعد مراجعة النيابة العمومية وذلك أثر تعمده نقل مجموعة من العمال يترواح عددهم قرابة 30شخص حيث تم ايداع السيارة المستودع البلدي. وقد تم ضبطهم على مستوى طريق منزل المحطة قادمين من إحدى...\\n2020/4/7 10:08\\nعلى امتداد الـ24 ساعة الأخيرة.. غلق عشرات المقاهي والمحلات والاحتفاظ بـ506 شخصا', 'قلع التيفاف توى الله غالب تعدى على راس خوكم سعد و الجرايد أعليك و بيك قلاب سراق مزور برى بيع الخضره', 'إذاعة قفصة | كاس رابطة ابطال افريقيا المجموعة 3 الجولة1:بداية متعثرة للافريقي امام النادي القسنطيني الجزائري 0- 1\\nكاس رابطة ابطال افريقيا المجموعة 3 الجولة1:بداية متعثرة للافريقي امام النادي القسنطيني الجزائري 0- 1\\nانقاد النادي الافريقي الى هزيمة مفاجئة مساء اليوم الجمعة بالملعب الاولمبي بسوسة امام النادي الرياضي القسنطيني الجزائري بنتيجة 1-صفر في مباراة الجولة الاولى من منافسات المجموعة الثالثة لكاس رابطة ابطال افريقيا .\\nتوقيت الإدراج ◔ 00:33 12.01.2019\\nآخر تحيين ◔ 00:33 12.01.2019']\n"
     ]
    }
   ],
   "source": [
    "# The Tunisiya Corpus is not on Hugging Face.\n",
    "# The user has provided an uploaded version of this.\n",
    "# We are now using available Hugging Face datasets.\n",
    "\n",
    "# Load and preprocess all specified datasets.\n",
    "loaded_datasets = load_and_preprocess_datasets()\n",
    "\n",
    "# Combine, deduplicate, and shuffle the datasets.\n",
    "final_corpus = combine_and_deduplicate(loaded_datasets)\n",
    "\n",
    "# Save the final prepared corpus.\n",
    "# save_dataset(final_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09aee97d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "785709"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c7d3b15",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mfinal_corpus\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'Dataset' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "final_corpus.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb8b255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
