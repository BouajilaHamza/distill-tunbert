{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5673949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Ensure matplotlib is set to use a non-interactive backend for saving files\n",
    "plt.switch_backend('Agg')\n",
    "\n",
    "# --- Configuration ---\n",
    "# Updated to load directly from the Hugging Face Hub\n",
    "DATASET_PATH = \"hamzabouajila/tunisian-derja-unified-raw-corpus\"\n",
    "OUTPUT_DIR = \"analysis_output\"\n",
    "TEXT_COLUMN_NAME = \"text\"\n",
    "\n",
    "# --- Main Analysis Functions ---\n",
    "\n",
    "def load_dataset_for_analysis():\n",
    "    \"\"\"\n",
    "    Loads the dataset directly from the Hugging Face Hub.\n",
    "    Returns:\n",
    "        A Hugging Face Dataset object or None if loading fails.\n",
    "    \"\"\"\n",
    "    print(f\"Loading dataset from Hugging Face: '{DATASET_PATH}'...\")\n",
    "    try:\n",
    "        # Load the 'train' split of the dataset\n",
    "        return load_dataset(DATASET_PATH, split='train')\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load dataset from Hugging Face: {e}\")\n",
    "        return None\n",
    "\n",
    "def analyze_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Performs a detailed analysis of the dataset.\n",
    "    Args:\n",
    "        dataset: The Hugging Face Dataset object.\n",
    "    Returns:\n",
    "        A dictionary of analysis insights.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Dataset Analysis ---\")\n",
    "    df = dataset.to_pandas()\n",
    "    \n",
    "    # 1. Basic statistics\n",
    "    total_rows = len(df)\n",
    "    unique_rows = df[TEXT_COLUMN_NAME].nunique()\n",
    "    null_values = df[TEXT_COLUMN_NAME].isnull().sum()\n",
    "    empty_strings = (df[TEXT_COLUMN_NAME].astype(str).str.strip() == '').sum()\n",
    "    \n",
    "    print(f\"Total entries: {total_rows}\")\n",
    "    print(f\"Unique entries: {unique_rows}\")\n",
    "    print(f\"Entries with Null values: {null_values}\")\n",
    "    print(f\"Entries with empty strings: {empty_strings}\")\n",
    "    \n",
    "    # 2. Text length analysis\n",
    "    df['text_length'] = df[TEXT_COLUMN_NAME].astype(str).apply(len)\n",
    "    df['word_count'] = df[TEXT_COLUMN_NAME].astype(str).apply(lambda x: len(re.findall(r'\\b\\w+\\b', x)))\n",
    "    \n",
    "    avg_length = df['text_length'].mean()\n",
    "    min_length = df['text_length'].min()\n",
    "    max_length = df['text_length'].max()\n",
    "    median_length = df['text_length'].median()\n",
    "    \n",
    "    avg_words = df['word_count'].mean()\n",
    "    min_words = df['word_count'].min()\n",
    "    max_words = df['word_count'].max()\n",
    "    \n",
    "    print(\"\\n--- Text Length Insights ---\")\n",
    "    print(f\"Average character length: {avg_length:.2f}\")\n",
    "    print(f\"Minimum character length: {min_length}\")\n",
    "    print(f\"Maximum character length: {max_length}\")\n",
    "    print(f\"Median character length: {median_length}\")\n",
    "    print(f\"Average word count: {avg_words:.2f}\")\n",
    "    print(f\"Minimum word count: {min_words}\")\n",
    "    print(f\"Maximum word count: {max_words}\")\n",
    "    \n",
    "    # 3. Most common words (after cleaning)\n",
    "    print(\"\\n--- Most Common Words ---\")\n",
    "    all_text = \" \".join(df[TEXT_COLUMN_NAME].astype(str).tolist())\n",
    "    # Simple tokenization and cleaning: remove non-alphabetic characters\n",
    "    words = re.findall(r'[\\u0600-\\u06FF\\w\\']+', all_text.lower())\n",
    "    word_counts = Counter(words)\n",
    "    most_common_words = word_counts.most_common(20)\n",
    "    for word, count in most_common_words:\n",
    "        print(f\"'{word}': {count}\")\n",
    "    \n",
    "    # 4. Save insights to a file\n",
    "    insights = {\n",
    "        'total_rows': total_rows,\n",
    "        'unique_rows': unique_rows,\n",
    "        'null_values': null_values,\n",
    "        'empty_strings': empty_strings,\n",
    "        'avg_length': avg_length,\n",
    "        'min_length': min_length,\n",
    "        'max_length': max_length,\n",
    "        'median_length': median_length,\n",
    "        'avg_words': avg_words,\n",
    "        'min_words': min_words,\n",
    "        'max_words': max_words,\n",
    "        'most_common_words': most_common_words\n",
    "    }\n",
    "    return df, insights\n",
    "\n",
    "def create_and_save_visualizations(df):\n",
    "    \"\"\"\n",
    "    Creates and saves visualizations of the dataset.\n",
    "    Args:\n",
    "        df: The pandas DataFrame of the dataset.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Generating Visualizations ---\")\n",
    "    \n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "        \n",
    "    # Plot 1: Histogram of text lengths\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df['text_length'], bins=50, kde=True, color='skyblue')\n",
    "    plt.title('Distribution of Text Length (Characters)')\n",
    "    plt.xlabel('Character Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'text_length_histogram.png'))\n",
    "    plt.show() # Added to display plot in Jupyter\n",
    "    plt.close()\n",
    "    print(f\"Saved histogram to {os.path.join(OUTPUT_DIR, 'text_length_histogram.png')}\")\n",
    "    \n",
    "    # Plot 2: Bar chart of top 20 most common words\n",
    "    all_text = \" \".join(df[TEXT_COLUMN_NAME].astype(str).tolist())\n",
    "    words = re.findall(r'[\\u0600-\\u06FF\\w\\']+', all_text.lower())\n",
    "    word_counts = Counter(words)\n",
    "    top_words_df = pd.DataFrame(word_counts.most_common(20), columns=['word', 'count'])\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='count', y='word', data=top_words_df, palette='viridis')\n",
    "    plt.title('Top 20 Most Frequent Words')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Word')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'top_words_barchart.png'))\n",
    "    plt.show() # Added to display plot in Jupyter\n",
    "    plt.close()\n",
    "    print(f\"Saved bar chart to {os.path.join(OUTPUT_DIR, 'top_words_barchart.png')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8028b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from Hugging Face: 'hamzabouajila/tunisian-derja-unified-raw-corpus'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d40693fd024bcabd603849b5a16e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/331 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be1595159774bcb8454fa4880456b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00002.parquet:   0%|          | 0.00/173M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8392d715435486f8222fc0975c9ecdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00002.parquet:   0%|          | 0.00/172M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1c5f6b895e45a5a64c8723bb276ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/802659 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Dataset Analysis ---\n",
      "Total entries: 802659\n",
      "Unique entries: 802658\n",
      "Entries with Null values: 1\n",
      "Entries with empty strings: 1\n",
      "\n",
      "--- Text Length Insights ---\n",
      "Average character length: 474.02\n",
      "Minimum character length: 0\n",
      "Maximum character length: 138325\n",
      "Median character length: 62.0\n",
      "Average word count: 81.62\n",
      "Minimum word count: 0\n",
      "Maximum word count: 25546\n",
      "\n",
      "--- Most Common Words ---\n",
      "'في': 1772354\n",
      "'من': 1301867\n",
      "'على': 879769\n",
      "'و': 646063\n",
      "'إلى': 386569\n",
      "'أن': 372829\n",
      "'ما': 305363\n",
      "'عن': 292773\n",
      "'التي': 272821\n",
      "'مع': 228677\n",
      "'تونس': 218745\n",
      "'لا': 211174\n",
      "'هذا': 188284\n",
      "'،': 177059\n",
      "'الذي': 159266\n",
      "'بعد': 155041\n",
      "'اليوم': 151762\n",
      "'هذه': 151142\n",
      "'بين': 149108\n",
      "'الى': 144039\n",
      "\n",
      "--- Generating Visualizations ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79039/2881169785.py:123: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show() # Added to display plot in Jupyter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved histogram to analysis_output/text_length_histogram.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79039/2881169785.py:134: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x='count', y='word', data=top_words_df, palette='viridis')\n",
      "/tmp/ipykernel_79039/2881169785.py:140: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show() # Added to display plot in Jupyter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved bar chart to analysis_output/top_words_barchart.png\n",
      "\n",
      "Analysis complete. Check the 'analysis_output' folder for insights and plots!\n"
     ]
    }
   ],
   "source": [
    "dataset_hf = load_dataset_for_analysis()\n",
    "\n",
    "if dataset_hf:\n",
    "    df_analysis, insights = analyze_dataset(dataset_hf)\n",
    "    create_and_save_visualizations(df_analysis)\n",
    "    print(f\"\\nAnalysis complete. Check the '{OUTPUT_DIR}' folder for insights and plots!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b78d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
