{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsZV-qASbMK2",
        "outputId": "3472df68-21a6-443c-dea7-352e2ad25680"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
            "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
            "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\n",
            "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\n",
            "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
            "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\n",
            "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\n",
            "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
            "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
            "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.5.1\n",
            "    Uninstalling fsspec-2025.5.1:\n",
            "      Successfully uninstalled fsspec-2025.5.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
            "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
            "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets accelerate sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import wandb\n",
        "import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "import torch.nn.functional as F\n",
        "import logging\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "load_dotenv()\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "if os.getenv(\"KAGGLE_CONTAINER_NAME\"):\n",
        "    from kaggle_secrets import UserSecretsClient\n",
        "    user_secrets = UserSecretsClient()\n",
        "    HF_TOKEN = user_secrets.get_secret(\"hamza_hf_token\")\n",
        "    WANDB_API_KEY = user_secrets.get_secret(\"wandb_api_key\")\n",
        "else:   \n",
        "    HF_TOKEN = os.getenv(\"HF_TOKEN\")  # Hugging Face token\n",
        "    WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")  # W&B API key\n",
        "\n",
        "# --- Configuration ---\n",
        "DATASET_NAME = \"hamzabouajila/tunisian-derja-unified-raw-corpus\" # Replace with your dataset name\n",
        "TEACHER_MODEL = \"not-lain/TunBERT\"\n",
        "STUDENT_MODEL = \"distilbert-base-uncased\"\n",
        "SAVE_DIR = \"/kaggle/working/distilled_tunbert\"\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "EVAL_BATCH_SIZE = 16\n",
        "NUM_TRAIN_EPOCHS = 3\n",
        "MAX_LENGTH = 128\n",
        "LEARNING_RATE = 5e-5\n",
        "WEIGHT_DECAY = 0.01\n",
        "WARMUP_STEPS = 100\n",
        "MAX_GRAD_NORM = 1.0\n",
        "TEMPERATURE = 2.0\n",
        "ALPHA_KD = 0.5\n",
        "ALPHA_MLM = 0.5\n",
        "ALPHA_HIDDEN = 0.0  # Set to 0 to disable hidden state loss for simplicity\n",
        "GRADIENT_ACCUMULATION_STEPS = 1\n",
        "HIDDEN_LAYERS_TO_MATCH = [-1]  # Match last hidden layer if used\n",
        "\n",
        "# --- Install Dependencies ---\n",
        "try:\n",
        "    import transformers\n",
        "    import datasets\n",
        "    import wandb\n",
        "except ImportError:\n",
        "    logger.info(\"Installing required libraries...\")\n",
        "    os.system(\"pip install transformers datasets wandb torch\")\n",
        "\n",
        "# --- W&B Initialization ---\n",
        "def init_wandb():\n",
        "    status = wandb.login(key=WANDB_API_KEY)\n",
        "    print(\"wandb logged in \", status)\n",
        "    wandb.init(\n",
        "        project=\"tunbert-distillation\",\n",
        "        config={\n",
        "            \"teacher_model\": TEACHER_MODEL,\n",
        "            \"student_model\": STUDENT_MODEL,\n",
        "            \"dataset\": DATASET_NAME,\n",
        "            \"train_batch_size\": TRAIN_BATCH_SIZE,\n",
        "            \"eval_batch_size\": EVAL_BATCH_SIZE,\n",
        "            \"num_train_epochs\": NUM_TRAIN_EPOCHS,\n",
        "            \"max_length\": MAX_LENGTH,\n",
        "            \"learning_rate\": LEARNING_RATE,\n",
        "            \"weight_decay\": WEIGHT_DECAY,\n",
        "            \"warmup_steps\": WARMUP_STEPS,\n",
        "            \"max_grad_norm\": MAX_GRAD_NORM,\n",
        "            \"temperature\": TEMPERATURE,\n",
        "            \"alpha_kd\": ALPHA_KD,\n",
        "            \"alpha_mlm\": ALPHA_MLM,\n",
        "            \"alpha_hidden\": ALPHA_HIDDEN,\n",
        "            \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION_STEPS,\n",
        "        },\n",
        "    )\n",
        "\n",
        "# --- Utilities ---\n",
        "def prepare_datasets():\n",
        "    logger.info(f\"Loading dataset: {DATASET_NAME}\")\n",
        "    dataset = load_dataset(DATASET_NAME, split=\"train\", token=HF_TOKEN)\n",
        "    # Split into train and validation if needed\n",
        "    dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "    return {\"train\": dataset[\"train\"], \"validation\": dataset[\"test\"]}\n",
        "\n",
        "def tokenize_function(examples, tokenizer):\n",
        "    texts = [str(text) if text is not None and str(text) != \"nan\" else \"\" for text in examples[\"text\"]]\n",
        "    return tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None,\n",
        "        return_special_tokens_mask=True,\n",
        "    )\n",
        "\n",
        "def group_texts(examples):\n",
        "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated[\"input_ids\"])\n",
        "    if total_length >= MAX_LENGTH:\n",
        "        total_length = (total_length // MAX_LENGTH) * MAX_LENGTH\n",
        "    result = {\n",
        "        k: [t[i : i + MAX_LENGTH] for i in range(0, total_length, MAX_LENGTH)]\n",
        "        for k, t in concatenated.items()\n",
        "    }\n",
        "    return result\n",
        "\n",
        "def mask_tokens(inputs, tokenizer, mlm_probability=0.15):\n",
        "    labels = inputs.clone()\n",
        "    probability_matrix = torch.full(labels.shape, mlm_probability)\n",
        "    special_tokens_mask = [\n",
        "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
        "    ]\n",
        "    special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
        "    probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
        "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "    labels[~masked_indices] = -100\n",
        "\n",
        "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
        "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
        "\n",
        "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
        "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
        "    inputs[indices_random] = random_words[indices_random]\n",
        "\n",
        "    return inputs, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize W&B\n",
        "init_wandb()\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logger.info(f\"Using device: {device}\")\n",
        "\n",
        "# Load tokenizer and models\n",
        "logger.info(f\"Loading tokenizer and teacher model: {TEACHER_MODEL}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(TEACHER_MODEL, token=HF_TOKEN, use_fast=True)\n",
        "teacher = AutoModelForMaskedLM.from_pretrained(TEACHER_MODEL, token=HF_TOKEN).to(device)\n",
        "teacher.eval()\n",
        "\n",
        "logger.info(f\"Initializing student model: {STUDENT_MODEL}\")\n",
        "student_config = AutoConfig.from_pretrained(STUDENT_MODEL)\n",
        "student = AutoModelForMaskedLM.from_config(student_config).to(device)\n",
        "try:\n",
        "    student = AutoModelForMaskedLM.from_pretrained(STUDENT_MODEL).to(device)\n",
        "except Exception:\n",
        "    logger.info(\"Using random initialization for student model\")\n",
        "student.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "865e4bcaec33498cad0381326f6fd7f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/722393 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "64356fe882ae41deb71ca200ad9f15de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/80266 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "# Load and preprocess dataset\n",
        "raw_datasets = prepare_datasets()\n",
        "tokenized_train = raw_datasets[\"train\"].map(\n",
        "    lambda x: tokenize_function(x, tokenizer),\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"train\"].column_names\n",
        ")\n",
        "tokenized_val = raw_datasets[\"validation\"].map(\n",
        "    lambda x: tokenize_function(x, tokenizer),\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"validation\"].column_names\n",
        ") if \"validation\" in raw_datasets else None\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.tensor([ex[\"input_ids\"] for ex in batch], dtype=torch.long)\n",
        "    attention_mask = torch.tensor([ex[\"attention_mask\"] for ex in batch], dtype=torch.long)\n",
        "    inputs_for_model, labels = mask_tokens(input_ids.clone(), tokenizer)\n",
        "    return {\"input_ids\": inputs_for_model, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "\n",
        "train_loader = DataLoader(tokenized_train, batch_size=TRAIN_BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(tokenized_val, batch_size=EVAL_BATCH_SIZE, shuffle=False, collate_fn=collate_fn) if tokenized_val else None\n",
        "\n",
        "# Optimizer and scheduler\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "optimizer_grouped_parameters = [\n",
        "    {\"params\": [p for n, p in student.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": WEIGHT_DECAY},\n",
        "    {\"params\": [p for n, p in student.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE)\n",
        "total_steps = (len(train_loader) // GRADIENT_ACCUMULATION_STEPS) * NUM_TRAIN_EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=total_steps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'module' object is not callable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_154/4014892726.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"total\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"kd\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mlm\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hidden\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mrunning_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "global_step = 0\n",
        "for epoch in range(NUM_TRAIN_EPOCHS):\n",
        "    student.train()\n",
        "    running_loss = {\"total\": 0.0, \"kd\": 0.0, \"mlm\": 0.0, \"hidden\": 0.0}\n",
        "    running_steps = 0\n",
        "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):\n",
        "        inputs = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Teacher forward\n",
        "        with torch.no_grad():\n",
        "            teacher_outputs = teacher(input_ids=inputs, attention_mask=attention_mask, output_hidden_states=ALPHA_HIDDEN > 0)\n",
        "            teacher_logits = teacher_outputs.logits\n",
        "            teacher_hidden = teacher_outputs.hidden_states if ALPHA_HIDDEN > 0 else None\n",
        "\n",
        "        # Student forward\n",
        "        student_outputs = student(input_ids=inputs, attention_mask=attention_mask, output_hidden_states=ALPHA_HIDDEN > 0)\n",
        "        student_logits = student_outputs.logits\n",
        "        student_hidden = student_outputs.hidden_states if ALPHA_HIDDEN > 0 else None\n",
        "\n",
        "        # KD loss\n",
        "        s_logits = student_logits.view(-1, student_logits.size(-1)) / TEMPERATURE\n",
        "        t_logits = teacher_logits.view(-1, teacher_logits.size(-1)) / TEMPERATURE\n",
        "        s_log_prob = F.log_softmax(s_logits, dim=-1)\n",
        "        t_prob = F.softmax(t_logits, dim=-1)\n",
        "        kd_loss = F.kl_div(s_log_prob, t_prob, reduction=\"batchmean\") * (TEMPERATURE ** 2)\n",
        "\n",
        "        # MLM loss\n",
        "        mlm_loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
        "        mlm_loss = mlm_loss_fct(student_logits.view(-1, student_logits.size(-1)), labels.view(-1))\n",
        "\n",
        "        # Total loss\n",
        "        total_loss = ALPHA_KD * kd_loss + ALPHA_MLM * mlm_loss\n",
        "\n",
        "        # Hidden state loss\n",
        "        hidden_loss = 0.0\n",
        "        if ALPHA_HIDDEN > 0 and student_hidden is not None and teacher_hidden is not None:\n",
        "            th = teacher_hidden[HIDDEN_LAYERS_TO_MATCH[0]]\n",
        "            sh = student_hidden[HIDDEN_LAYERS_TO_MATCH[0]]\n",
        "            if th.shape != sh.shape:\n",
        "                proj = torch.nn.Linear(sh.size(-1), th.size(-1)).to(device)\n",
        "                sh_proj = proj(sh)\n",
        "                hidden_loss = F.mse_loss(sh_proj, th)\n",
        "            else:\n",
        "                hidden_loss = F.mse_loss(sh, th)\n",
        "            total_loss = total_loss + ALPHA_HIDDEN * hidden_loss\n",
        "\n",
        "        # Backprop\n",
        "        total_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(student.parameters(), MAX_GRAD_NORM)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Log metrics\n",
        "        running_loss[\"total\"] += total_loss.item()\n",
        "        running_loss[\"kd\"] += kd_loss.item()\n",
        "        running_loss[\"mlm\"] += mlm_loss.item()\n",
        "        running_loss[\"hidden\"] += hidden_loss.item() if ALPHA_HIDDEN > 0 else 0.0\n",
        "        running_steps += 1\n",
        "\n",
        "        if global_step % 100 == 0:\n",
        "            avg_loss = {k: v / running_steps for k, v in running_loss.items()}\n",
        "            wandb.log({\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"step\": global_step,\n",
        "                \"total_loss\": avg_loss[\"total\"],\n",
        "                \"kd_loss\": avg_loss[\"kd\"],\n",
        "                \"mlm_loss\": avg_loss[\"mlm\"],\n",
        "                \"hidden_loss\": avg_loss[\"hidden\"] if ALPHA_HIDDEN > 0 else 0.0,\n",
        "            })\n",
        "            logger.info(f\"Epoch {epoch+1} Step {global_step} TotalLoss {avg_loss['total']:.4f} KDLoss {avg_loss['kd']:.4f} MLMLoss {avg_loss['mlm']:.4f}\")\n",
        "            running_loss = {k: 0.0 for k in running_loss}\n",
        "            running_steps = 0\n",
        "\n",
        "        global_step += 1\n",
        "\n",
        "    # Validation\n",
        "    if val_loader is not None:\n",
        "        student.eval()\n",
        "        eval_loss = 0.0\n",
        "        eval_steps = 0\n",
        "        with torch.no_grad():\n",
        "            for vb in tqdm(val_loader, desc=\"Validation\"):\n",
        "                v_inputs = vb[\"input_ids\"].to(device)\n",
        "                v_attention = vb[\"attention_mask\"].to(device)\n",
        "                v_labels = vb[\"labels\"].to(device)\n",
        "                v_student_out = student(input_ids=v_inputs, attention_mask=v_attention)\n",
        "                v_logits = v_student_out.logits\n",
        "                v_loss = mlm_loss_fct(v_logits.view(-1, v_logits.size(-1)), v_labels.view(-1)).item()\n",
        "                eval_loss += v_loss\n",
        "                eval_steps += 1\n",
        "        avg_eval_loss = eval_loss / eval_steps\n",
        "        wandb.log({\"epoch\": epoch + 1, \"validation_mlm_loss\": avg_eval_loss})\n",
        "        logger.info(f\"Validation MLM loss after epoch {epoch+1}: {avg_eval_loss:.4f}\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    checkpoint_dir = os.path.join(SAVE_DIR, f\"student-epoch-{epoch+1}\")\n",
        "    student.save_pretrained(checkpoint_dir)\n",
        "    tokenizer.save_pretrained(checkpoint_dir)\n",
        "    wandb.save(checkpoint_dir + \"/*\")\n",
        "    logger.info(f\"Saved checkpoint to {checkpoint_dir}\")\n",
        "\n",
        "# Final save\n",
        "student.save_pretrained(SAVE_DIR)\n",
        "tokenizer.save_pretrained(SAVE_DIR)\n",
        "wandb.save(SAVE_DIR + \"/*\")\n",
        "logger.info(f\"Distillation finished. Model saved to {SAVE_DIR}\")\n",
        "\n",
        "# Push to Hugging Face Hub\n",
        "logger.info(\"Pushing model to Hugging Face Hub\")\n",
        "student.push_to_hub(\"hamzabouajila/distilled_tunbert\", use_auth_token=HF_TOKEN)\n",
        "tokenizer.push_to_hub(\"hamzabouajila/distilled_tunbert\", use_auth_token=HF_TOKEN)\n",
        "\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
